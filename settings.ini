# Используемые параметры
# [Model] 
# model (По умолчанию t-tech/T-lite-it-1.0)
# max_new_tokens (По умолчанию 512)
# top_k (По умолчанию 50)
# top_p (По умолчанию 0.85)
# temperature (По умолчанию 0.3)
# gpu_memory_utilization (По умолчанию 0.1)
# use_swap_memory (По умолчанию False)
# swap_space_size (По умолчанию 64GiB)

# [Path_to_PDFS]
# path (По умолчанию PDFS)

# [RAG]
# top_k (По умолчанию 8)
# chunk_size (По умолчанию 2000)
# chunk_overlap (По умолчанию 300)

# [BOT]
# token (Токен бота телеграмм)



# Настройки параметров модели в vllm
[Model]
# Модель для скачивания (Нужно быть уверенным, что к модели есть доступ)
model=ibm-granite/granite-embedding-30m-english
# Максимальное количество генерируемых моделью токенов
max_new_tokens=512
top_k=50
top_p=0.85
# 0-inf чем меньше значение, тем более консервативна модель, выбирает более вероятные токены.
temperature=0.35
# Количество видеопамяти используемое библиотекой. (Скорее всего, всегда будет больше использовать, так как модель требует какой-то минимум)
# Возможно, данный параметр в langchain vllm забагован, так как всё равно выдаёт 0.9 всегда.
gpu_memory_utilization=0.1
# Включить параметр подмены видеопамяти на обычную. (Возможно помогает, если видеопамяти недостаточно для модели)
use_swap_memory=False
# Количество памяти используемое вместо видеопамяти для содержания модели
swap_space_size=64GiB
[Path_to_PDFS]
# Путь до pdf в которых находится контекст для LLM
path=PDFS
[RAG]
# Количество выбираемых верхних результатов rag.
top_k=8
# Размер кусков RAG (Следует работать очень аккуратно с данным параметром)
chunk_size=2000
# Размер пересечения кусков RAG (Следует работать очень аккуратно с данным параметром)
chunk_overlap=300
[BOT]
# Токен бота
token=