# Используемые параметры
# [Model] 
# model (По умолчанию t-tech/T-lite-it-1.0)
# max_new_tokens (По умолчанию 512)
# top_k (По умолчанию 50)
# top_p (По умолчанию 0.85)
# temperature (По умолчанию 0.3)
# gpu_memory_utilization (По умолчанию 0.1)
# use_swap_memory (По умолчанию False)
# swap_space_size (По умолчанию 64GiB)

# [Path_to_PDFS]
# path (По умолчанию PDFS)

# [Instruction_for_embedder]
# instruction (По умолчанию Represent the query for retrieval: )

# [RAG]
# top_k (По умолчанию 5)
# chunk_size (По умолчанию 6000)
# chunk_size (По умолчанию 1500)



# Настройки параметров модели в vllm
[Model]
# Модель для скачивания (Нужно быть уверенным, что к модели есть доступ)
model=t-tech/T-lite-it-1.0
# Максимальное количество генерируемых моделью токенов
max_new_tokens=1024
top_k=10
top_p=0.95
# 0-inf чем меньше значение, тем более консервативна модель, выбирает более вероятные токены.
temperature=0.8
# Количество видеопамяти используемое библиотекой. (Скорее всего, всегда будет больше использовать, так как модель требует какой-то минимум)
# Возможно, данный параметр в langchain vllm забагован, так как всё равно выдаёт 0.9 всегда.
gpu_memory_utilization=0.1
# Включить параметр подмены видеопамяти на обычную. (Возможно помогает, если видеопамяти недостаточно для модели)
use_swap_memory=True
# Количество памяти используемое вместо видеопамяти для содержания модели
swap_space_size=64GiB
[Path_to_PDFS]
# Путь до pdf в которых находится контекст для LLM
path=PDFS
[Instruction_for_embedder]
# Инструкция для эмбеддера (RAG) (Следует работать очень аккуратно с данным параметром)
instruction=Represent the query for retrieval: 
[RAG]
# Количество выбираемых верхних результатов rag.
top_k=10
# Размер кусков RAG (Следует работать очень аккуратно с данным параметром)
chunk_size=6000
# Размер пересечения кусков RAG (Следует работать очень аккуратно с данным параметром)
chunk_overlap=1500